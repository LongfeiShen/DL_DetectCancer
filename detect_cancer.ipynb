{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKgJXlqFMid3"
      },
      "source": [
        "## mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viORSD7wMjy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedfafcc-ceb3-4939-8897-a2619a35464f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYjMtPwLMwUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58aa40c0-a490-4441-80e3-03d2e11b4dd7"
      },
      "source": [
        "import os, shutil\n",
        "path = \"/content/drive/My Drive/DL_project\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['slides',\n",
              " 'patches_and_labels_sm',\n",
              " 'patches_and_labels',\n",
              " 'dataset_L2',\n",
              " 'dataset_L3',\n",
              " 'dataset_L4',\n",
              " 'dataset_L5',\n",
              " 'my_model',\n",
              " 'patches_and_labels (1)',\n",
              " 'my_model_dropout_trainable',\n",
              " 'model_multiple_zoom',\n",
              " 'my_model_pretrained',\n",
              " 'my_model_not_pretrained']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AECM4vMHMenr"
      },
      "source": [
        "# project start\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUksKKoRMY49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135e3af5-f388-474d-dc74-7abea75a88b5"
      },
      "source": [
        "# Install the OpenSlide C library and Python bindings\n",
        "# After installing these libraries, use `Runtime -> restart and run all` on the menu\n",
        "!apt-get install openslide-tools\n",
        "!pip install openslide-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openslide-tools is already the newest version (3.4.1+dfsg-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: openslide-python in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from openslide-python) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M7chksMMY5D"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from openslide import open_slide, __library_version__ as openslide_version\n",
        "import os\n",
        "from PIL import Image\n",
        "from skimage.color import rgb2gray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbhLeWquMY5E"
      },
      "source": [
        "# See https://openslide.org/api/python/#openslide.OpenSlide.read_region\n",
        "# Note: x,y coords are with respect to level 0.\n",
        "# There is an example below of working with coordinates\n",
        "# with respect to a higher zoom level.\n",
        "\n",
        "# Read a region from the slide\n",
        "# Return a numpy RBG array\n",
        "def read_slide(slide, x, y, level, width, height, as_float=False):\n",
        "    im = slide.read_region((x,y), level, (width, height))\n",
        "    im = im.convert('RGB') # drop the alpha channel\n",
        "    if as_float:\n",
        "        im = np.asarray(im, dtype=np.float32)\n",
        "    else:\n",
        "        im = np.asarray(im)\n",
        "    assert im.shape == (height, width, 3)\n",
        "    return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKXrvO_jMY5E"
      },
      "source": [
        "# As mentioned in class, we can improve efficiency by ignoring non-tissue areas \n",
        "# of the slide. We'll find these by looking for all gray regions.\n",
        "def find_tissue_pixels(image, intensity=0.8):\n",
        "    im_gray = rgb2gray(image)\n",
        "    assert im_gray.shape == (image.shape[0], image.shape[1])\n",
        "    indices = np.where(im_gray <= intensity)\n",
        "    return list(zip(indices[0], indices[1]))\n",
        "\n",
        "# tissue_pixels = find_tissue_pixels(slide_image)\n",
        "# percent_tissue = len(tissue_pixels) / float(slide_image.shape[0] * slide_image.shape[0]) * 100\n",
        "# print (\"%d tissue_pixels pixels (%.1f percent of the image)\" % (len(tissue_pixels), percent_tissue)) \n",
        "\n",
        "def apply_mask(im, mask, color=(255,0,0)):\n",
        "    masked = np.copy(im)\n",
        "    for x,y in mask: masked[x][y] = color\n",
        "    return masked\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Wxuuz1MY5F"
      },
      "source": [
        "## Preparations (data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXc07pORMY5F"
      },
      "source": [
        "# slides split\n",
        "slides_idx = ['001', '002', '005', '012', '016', '019', '023', '031', '035', '057',\n",
        "              '059', '064', '075','078', '081', '091', '094', '096', '101', '110']\n",
        "\n",
        "slide_paths = ['slides/tumor_{}.tif'.format(i) for i in slides_idx]\n",
        "tumor_mask_paths = ['slides/tumor_{}_mask.tif'.format(i) for i in slides_idx]\n",
        "\n",
        "# 70% train, 15% test, 15 validation\n",
        "\n",
        "train_slide_paths, test_slide_paths, validation_slide_paths = slide_paths[:14], slide_paths[14:17], slide_paths[17:]\n",
        "train_tumor_mask_paths = tumor_mask_paths[:14]\n",
        "test_tumor_mask_paths = tumor_mask_paths[14:17]\n",
        "validation_tumor_mask_paths = tumor_mask_paths[17:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REkxqA4wMY5F"
      },
      "source": [
        "import random\n",
        "\n",
        "def get_patches_and_labels(slide_path, tumor_mask_path, level=2):\n",
        "    # save patches of different resolution in different directories.\n",
        "    name = slide_path.split('/')[-1][:-4]\n",
        "    main_dir = 'patches_and_labels'\n",
        "    if not os.path.exists(main_dir):\n",
        "        os.mkdir(main_dir)\n",
        "    slide_dir = os.path.join(main_dir, name)\n",
        "    if not os.path.exists(slide_dir):\n",
        "        os.mkdir(slide_dir)\n",
        "    slide_pos_dir = os.path.join(slide_dir, 'positive')\n",
        "    slide_pos_dir_L2 = os.path.join(slide_pos_dir, 'L2')\n",
        "    slide_pos_dir_L3 = os.path.join(slide_pos_dir, 'L3')\n",
        "    slide_pos_dir_L4 = os.path.join(slide_pos_dir, 'L4')\n",
        "    slide_pos_dir_L5 = os.path.join(slide_pos_dir, 'L5')\n",
        "    slide_neg_dir = os.path.join(slide_dir, 'negative')\n",
        "    slide_neg_dir_L2 = os.path.join(slide_neg_dir, 'L2')\n",
        "    slide_neg_dir_L3 = os.path.join(slide_neg_dir, 'L3')\n",
        "    slide_neg_dir_L4 = os.path.join(slide_neg_dir, 'L4')\n",
        "    slide_neg_dir_L5 = os.path.join(slide_neg_dir, 'L5')\n",
        "    for dir_path in [slide_pos_dir, slide_pos_dir_L2, slide_pos_dir_L3, slide_pos_dir_L4, slide_pos_dir_L5,\n",
        "                    slide_neg_dir, slide_neg_dir_L2, slide_neg_dir_L3, slide_neg_dir_L4, slide_neg_dir_L5]:\n",
        "        if not os.path.exists(dir_path):\n",
        "            os.mkdir(dir_path)\n",
        "    ######\n",
        "    slide = open_slide(slide_path)\n",
        "    tumor_mask = open_slide(tumor_mask_path)\n",
        "    # slide a window across the slide\n",
        "    window_width = 299\n",
        "    window_height = 299\n",
        "    stride = 128\n",
        "    # \n",
        "    slide_width, slide_height = slide.level_dimensions[level]\n",
        "    # num of grids\n",
        "    nx = (slide_width - window_width) // stride + 1\n",
        "    ny = (slide_height - window_height) // stride + 1\n",
        "    print(nx, ny)\n",
        "    kk = 0\n",
        "    for i in range(ny):\n",
        "        for j in range(nx):\n",
        "            x, y = j * stride, i * stride\n",
        "            region = read_slide(slide, x=x * 2**level, y = y * 2**level, \n",
        "                               level=level, width=window_width, height=window_height)\n",
        "#             plt.imshow(region)\n",
        "#             plt.show()\n",
        "            # find if it contains tissue\n",
        "            tissue_pixels = find_tissue_pixels(region)\n",
        "            if len(tissue_pixels) > 299*299/100*2:\n",
        "                # tissue pixels percentage larger than 2%, this region has tissue\n",
        "                # get label of this region\n",
        "                mask_region = read_slide(tumor_mask, x=x * 2**level, y = y * 2**level, \n",
        "                               level=level, width=window_width, height=window_height)[:,:,0]\n",
        "                label = mask_region[86:214, 86:214].max()\n",
        "                # if label=1, it is positive, else, negative\n",
        "                \n",
        "                # prepare zoom out patches, same center\n",
        "                ############################\n",
        "                level2 = level + 1\n",
        "                level3 = level + 2\n",
        "                level4 = level + 3\n",
        "#          \n",
        "                ############################\n",
        "               \n",
        "                #\n",
        "                if label==1: # positive\n",
        "                    region2 = read_slide(slide, x=(x - window_width//2)* 2**level,\n",
        "                                    y=(y- window_height//2)*2**level, level=level2,\n",
        "                                    width=window_width, height=window_height)\n",
        "                    region3 = read_slide(slide, x=(x - window_width//2*3)* 2**level,\n",
        "                                    y=(y-window_height//2*3)*2**level, level=level3,\n",
        "                                    width=window_width, height=window_height)\n",
        "                    region4 = read_slide(slide, x=(x - window_width//2*7)* 2**level,\n",
        "                                    y=(y-window_height//2*7)*2**level, level=level4,\n",
        "                                    width=window_width, height=window_height)\n",
        "                    region = Image.fromarray(region)\n",
        "                    region2 = Image.fromarray(region2)\n",
        "                    region3 = Image.fromarray(region3)\n",
        "                    region4 = Image.fromarray(region4)\n",
        "                    region.save(slide_pos_dir_L2 + '/{}.png'.format(kk))\n",
        "                    region2.save(slide_pos_dir_L3 + '/{}.png'.format(kk))\n",
        "                    region3.save(slide_pos_dir_L4 + '/{}.png'.format(kk))\n",
        "                    region4.save(slide_pos_dir_L5 + '/{}.png'.format(kk))\n",
        "                    kk += 1\n",
        "                elif label==0:  # negative\n",
        "                    if random.random() < 0.9:\n",
        "                        region2 = read_slide(slide, x=(x - window_width//2)* 2**level,\n",
        "                                    y=(y- window_height//2)*2**level, level=level2,\n",
        "                                    width=window_width, height=window_height)\n",
        "                        region3 = read_slide(slide, x=(x - window_width//2*3)* 2**level,\n",
        "                                    y=(y-window_height//2*3)*2**level, level=level3,\n",
        "                                    width=window_width, height=window_height)\n",
        "                        region4 = read_slide(slide, x=(x - window_width//2*7)* 2**level,\n",
        "                                    y=(y-window_height//2*7)*2**level, level=level4,\n",
        "                                    width=window_width, height=window_height)\n",
        "                        region = Image.fromarray(region)\n",
        "                        region2 = Image.fromarray(region2)\n",
        "                        region3 = Image.fromarray(region3)\n",
        "                        region4 = Image.fromarray(region4)\n",
        "                        region.save(slide_neg_dir_L2 + '/{}.png'.format(kk))\n",
        "                        region2.save(slide_neg_dir_L3 + '/{}.png'.format(kk))\n",
        "                        region3.save(slide_neg_dir_L4 + '/{}.png'.format(kk))\n",
        "                        region4.save(slide_neg_dir_L5 + '/{}.png'.format(kk))\n",
        "                        kk += 1\n",
        "\n",
        "\n",
        "                    \n",
        "\n",
        "\n",
        "                \n",
        "    \n",
        "     \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndt0bNsuMY5G"
      },
      "source": [
        "# get patches and labels from all slides\n",
        "\n",
        "# for i in range(20):\n",
        "    # slide_path = slide_paths[i]\n",
        "    # tumor_mask_path = tumor_mask_paths[i]\n",
        "    # get_patches_and_labels(slide_path, tumor_mask_path, level=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tmedlMCZpqt"
      },
      "source": [
        "from random import choices\n",
        "import shutil\n",
        "\n",
        "\"\"\" In this part, i create a balanced dataset to avoid always predicting negative\"\"\"\n",
        "\n",
        "def create_dataset():\n",
        "    # create dir\n",
        "    for i in ['L2', 'L3', 'L4', 'L5']:\n",
        "        dir_name = 'dataset_{}'.format(i) \n",
        "        if not os.path.exists(dir_name):\n",
        "            os.mkdir(dir_name)\n",
        "            for k in ['train', 'train/positive', 'train/negative',\n",
        "                     'validation', 'validation/positive', 'validation/negative']:\n",
        "                os.mkdir(os.path.join(dir_name, k))\n",
        "    \n",
        "    train_dir = ['001', '002', '005', '012', '016', '019', '023', '031', '035', '057', '059', '064', '075', '078']\n",
        "    validation_dir = ['081', '091', '094']\n",
        "    # test_dir = ['096', '101', '110'] # these three slides are used for testing, not used here\n",
        "\n",
        "    train_dir = ['patches_and_labels/tumor_{}'.format(i) for i in train_dir]\n",
        "    validation_dir = ['patches_and_labels/tumor_{}'.format(i) for i in validation_dir]\n",
        "    \n",
        "    kk = 1\n",
        "    # \n",
        "    for i in train_dir + validation_dir:\n",
        "        pos_path = os.path.join(i, 'positive')\n",
        "        neg_path = os.path.join(i, 'negative')\n",
        "        # L2\n",
        "        pos_L2 = os.path.join(pos_path, 'L2')\n",
        "        neg_L2 = os.path.join(neg_path, 'L2')\n",
        "        list_pos_L2 = os.listdir(pos_L2)\n",
        "        list_pos_L2 = [n for n in list_pos_L2 if n != '.DS_Store']\n",
        "        list_neg_L2 = os.listdir(neg_L2)\n",
        "        list_neg_L2 = [n for n in list_neg_L2 if n != '.DS_Store']\n",
        "        list_neg_L2 = choices(list_neg_L2, k=len(list_pos_L2))\n",
        "        \n",
        "        # \n",
        "        pos_L3 = os.path.join(pos_path, 'L3')\n",
        "        neg_L3 = os.path.join(neg_path, 'L3')\n",
        "        \n",
        "        pos_L4 = os.path.join(pos_path, 'L4')\n",
        "        neg_L4 = os.path.join(neg_path, 'L4')\n",
        "        \n",
        "        pos_L5 = os.path.join(pos_path, 'L5')\n",
        "        neg_L5 = os.path.join(neg_path, 'L5')\n",
        "        for j in range(len(list_pos_L2)):\n",
        "            pos_name = list_pos_L2[j]\n",
        "            neg_name = list_neg_L2[j]\n",
        "            pos_L2_path = os.path.join(pos_L2, pos_name)\n",
        "            pos_L3_path = os.path.join(pos_L3, pos_name)\n",
        "            pos_L4_path = os.path.join(pos_L4, pos_name)\n",
        "            pos_L5_path = os.path.join(pos_L5, pos_name)\n",
        "            \n",
        "            neg_L2_path = os.path.join(neg_L2, neg_name)\n",
        "            neg_L3_path = os.path.join(neg_L3, neg_name)\n",
        "            neg_L4_path = os.path.join(neg_L4, neg_name)\n",
        "            neg_L5_path = os.path.join(neg_L5, neg_name)\n",
        "            # \n",
        "            if i in train_dir:\n",
        "                L2_new_pos_path = 'dataset_L2/train/positive'\n",
        "                L2_new_neg_path = 'dataset_L2/train/negative'\n",
        "                L3_new_pos_path = 'dataset_L3/train/positive'\n",
        "                L3_new_neg_path = 'dataset_L3/train/negative'\n",
        "                L4_new_pos_path = 'dataset_L4/train/positive'\n",
        "                L4_new_neg_path = 'dataset_L4/train/negative'\n",
        "                L5_new_pos_path = 'dataset_L5/train/positive'\n",
        "                L5_new_neg_path = 'dataset_L5/train/negative'\n",
        "            elif i in validation_dir:\n",
        "                L2_new_pos_path = 'dataset_L2/validation/positive'\n",
        "                L2_new_neg_path = 'dataset_L2/validation/negative'\n",
        "                L3_new_pos_path = 'dataset_L3/validation/positive'\n",
        "                L3_new_neg_path = 'dataset_L3/validation/negative'\n",
        "                L4_new_pos_path = 'dataset_L4/validation/positive'\n",
        "                L4_new_neg_path = 'dataset_L4/validation/negative'\n",
        "                L5_new_pos_path = 'dataset_L5/validation/positive'\n",
        "                L5_new_neg_path = 'dataset_L5/validation/negative'\n",
        "            \n",
        "            \n",
        "            shutil.copyfile(pos_L2_path, L2_new_pos_path+'/{}.png'.format(kk))\n",
        "            shutil.copyfile(pos_L3_path, L3_new_pos_path+'/{}.png'.format(kk))\n",
        "            shutil.copyfile(pos_L4_path, L4_new_pos_path+'/{}.png'.format(kk))\n",
        "            shutil.copyfile(pos_L5_path, L5_new_pos_path+'/{}.png'.format(kk))\n",
        "            shutil.copyfile(neg_L2_path, L2_new_neg_path+'/{}.png'.format(kk))\n",
        "            shutil.copyfile(neg_L3_path, L3_new_neg_path+'/{}.png'.format(kk))\n",
        "            shutil.copyfile(neg_L4_path, L4_new_neg_path+'/{}.png'.format(kk))\n",
        "            shutil.copyfile(neg_L5_path, L5_new_neg_path+'/{}.png'.format(kk))\n",
        "            \n",
        "            \n",
        "            kk += 1\n",
        "        print(i)\n",
        "    \n",
        "# create_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz7cZB7yaVNp"
      },
      "source": [
        "## Train with single, high magnification (L2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8orJP1mYgCsA"
      },
      "source": [
        "### Using data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1DYQJOFgyGw",
        "outputId": "5e912fb1-a7e9-455d-e29a-7d1ed27be78b"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "train_dir = 'dataset_L2/train/'\n",
        "validation_dir = 'dataset_L2/validation/'\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(  # flip and rotation used \n",
        "    rescale=1./255,\n",
        "    rotation_range=90,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(299, 299),\n",
        "        batch_size=128,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(299, 299),\n",
        "        batch_size=128,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15450 images belonging to 2 classes.\n",
            "Found 1160 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuTclf1X0tck"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wk6myn6-VLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a19053-6d11-4ade-f51d-f29b3c9e0182"
      },
      "source": [
        "# gpu\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "# model inception V3\n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(299, 299, 3),\n",
        "                                                 include_top=False, \n",
        "                                                #  weights='imagenet'\n",
        "                                                 weights=None\n",
        "                                               )\n",
        "base_model.trainable = True #False\n",
        "\n",
        "# \n",
        "dropout_layer = layers.Dropout(0.5)\n",
        "gloabal_average_layer = layers.GlobalAveragePooling2D()\n",
        "prediction_layer = layers.Dense(1, activation='sigmoid')\n",
        "###\n",
        "\n",
        "my_model = models.Sequential((base_model, \n",
        "                              dropout_layer,\n",
        "                            gloabal_average_layer, \n",
        "                              prediction_layer))\n",
        "\n",
        "my_model.compile(loss='binary_crossentropy',\n",
        "              # metrics=['acc'],\n",
        "              metrics=['acc', tf.keras.metrics.AUC()],\n",
        "              optimizer='sgd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 1273353831338846268\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15469833088\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 3389825494285277341\n",
            "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GMU2EG0vs6"
      },
      "source": [
        "### train (inception v3 not pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2AKq9uxghEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381f028e-1855-4261-e3e2-fdbc043dc4ed"
      },
      "source": [
        "history = my_model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=121,\n",
        "      epochs=40,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=10)\n",
        "\n",
        "# \n",
        "my_model.save(\"my_model_not_pretrained\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "121/121 [==============================] - 6924s 57s/step - loss: 0.4199 - acc: 0.8097 - auc: 0.8958 - val_loss: 0.7152 - val_acc: 0.5000 - val_auc: 0.7739\n",
            "Epoch 2/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.2480 - acc: 0.8914 - auc: 0.9624 - val_loss: 0.6944 - val_acc: 0.7974 - val_auc: 0.8225\n",
            "Epoch 3/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.1986 - acc: 0.9180 - auc: 0.9757 - val_loss: 1.1790 - val_acc: 0.8103 - val_auc: 0.8497\n",
            "Epoch 4/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.1666 - acc: 0.9313 - auc: 0.9833 - val_loss: 0.9189 - val_acc: 0.8259 - val_auc: 0.8942\n",
            "Epoch 5/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.1593 - acc: 0.9370 - auc: 0.9840 - val_loss: 1.7224 - val_acc: 0.6129 - val_auc: 0.6041\n",
            "Epoch 6/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.1436 - acc: 0.9451 - auc: 0.9867 - val_loss: 0.9162 - val_acc: 0.6284 - val_auc: 0.9146\n",
            "Epoch 7/40\n",
            "121/121 [==============================] - 317s 3s/step - loss: 0.1280 - acc: 0.9510 - auc: 0.9895 - val_loss: 0.5486 - val_acc: 0.7397 - val_auc: 0.9495\n",
            "Epoch 8/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.1085 - acc: 0.9583 - auc: 0.9925 - val_loss: 1.6089 - val_acc: 0.5448 - val_auc: 0.8608\n",
            "Epoch 9/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.1110 - acc: 0.9583 - auc: 0.9919 - val_loss: 0.7952 - val_acc: 0.7017 - val_auc: 0.8844\n",
            "Epoch 10/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.1056 - acc: 0.9594 - auc: 0.9925 - val_loss: 0.7705 - val_acc: 0.8362 - val_auc: 0.9099\n",
            "Epoch 11/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0948 - acc: 0.9627 - auc: 0.9942 - val_loss: 0.7622 - val_acc: 0.7009 - val_auc: 0.8833\n",
            "Epoch 12/40\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0974 - acc: 0.9632 - auc: 0.9941 - val_loss: 0.8009 - val_acc: 0.6371 - val_auc: 0.9567\n",
            "Epoch 13/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0888 - acc: 0.9631 - auc: 0.9952 - val_loss: 1.5137 - val_acc: 0.5647 - val_auc: 0.7547\n",
            "Epoch 14/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0928 - acc: 0.9642 - auc: 0.9945 - val_loss: 0.4191 - val_acc: 0.8043 - val_auc: 0.9586\n",
            "Epoch 15/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0848 - acc: 0.9679 - auc: 0.9954 - val_loss: 0.4301 - val_acc: 0.7940 - val_auc: 0.9498\n",
            "Epoch 16/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0745 - acc: 0.9713 - auc: 0.9966 - val_loss: 0.4473 - val_acc: 0.8276 - val_auc: 0.9161\n",
            "Epoch 17/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0815 - acc: 0.9693 - auc: 0.9954 - val_loss: 0.9783 - val_acc: 0.6922 - val_auc: 0.9079\n",
            "Epoch 18/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0838 - acc: 0.9683 - auc: 0.9954 - val_loss: 0.6766 - val_acc: 0.7293 - val_auc: 0.9371\n",
            "Epoch 19/40\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0742 - acc: 0.9699 - auc: 0.9967 - val_loss: 0.7758 - val_acc: 0.7164 - val_auc: 0.9336\n",
            "Epoch 20/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0785 - acc: 0.9703 - auc: 0.9957 - val_loss: 0.3049 - val_acc: 0.8879 - val_auc: 0.9515\n",
            "Epoch 21/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0788 - acc: 0.9706 - auc: 0.9960 - val_loss: 0.6287 - val_acc: 0.7448 - val_auc: 0.9337\n",
            "Epoch 22/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0684 - acc: 0.9750 - auc: 0.9968 - val_loss: 0.3311 - val_acc: 0.8966 - val_auc: 0.9578\n",
            "Epoch 23/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0641 - acc: 0.9741 - auc: 0.9974 - val_loss: 0.3513 - val_acc: 0.8836 - val_auc: 0.9468\n",
            "Epoch 24/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0733 - acc: 0.9720 - auc: 0.9966 - val_loss: 0.2910 - val_acc: 0.8948 - val_auc: 0.9700\n",
            "Epoch 25/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0677 - acc: 0.9749 - auc: 0.9968 - val_loss: 0.3551 - val_acc: 0.8897 - val_auc: 0.9499\n",
            "Epoch 26/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0713 - acc: 0.9699 - auc: 0.9968 - val_loss: 1.2117 - val_acc: 0.6138 - val_auc: 0.8816\n",
            "Epoch 27/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0646 - acc: 0.9736 - auc: 0.9972 - val_loss: 0.7613 - val_acc: 0.7112 - val_auc: 0.9260\n",
            "Epoch 28/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0595 - acc: 0.9769 - auc: 0.9978 - val_loss: 0.6959 - val_acc: 0.7069 - val_auc: 0.9437\n",
            "Epoch 29/40\n",
            "121/121 [==============================] - 317s 3s/step - loss: 0.0605 - acc: 0.9777 - auc: 0.9972 - val_loss: 0.5708 - val_acc: 0.7793 - val_auc: 0.9437\n",
            "Epoch 30/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0596 - acc: 0.9781 - auc: 0.9974 - val_loss: 0.3144 - val_acc: 0.8845 - val_auc: 0.9763\n",
            "Epoch 31/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0564 - acc: 0.9798 - auc: 0.9977 - val_loss: 0.3250 - val_acc: 0.8879 - val_auc: 0.9501\n",
            "Epoch 32/40\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0582 - acc: 0.9802 - auc: 0.9974 - val_loss: 0.3714 - val_acc: 0.8483 - val_auc: 0.9617\n",
            "Epoch 33/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0594 - acc: 0.9792 - auc: 0.9976 - val_loss: 1.4942 - val_acc: 0.6517 - val_auc: 0.8500\n",
            "Epoch 34/40\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0553 - acc: 0.9775 - auc: 0.9979 - val_loss: 0.6168 - val_acc: 0.7483 - val_auc: 0.9463\n",
            "Epoch 35/40\n",
            "121/121 [==============================] - 318s 3s/step - loss: 0.0528 - acc: 0.9799 - auc: 0.9981 - val_loss: 1.5644 - val_acc: 0.6629 - val_auc: 0.7580\n",
            "Epoch 36/40\n",
            "121/121 [==============================] - 322s 3s/step - loss: 0.0542 - acc: 0.9799 - auc: 0.9980 - val_loss: 0.2517 - val_acc: 0.9112 - val_auc: 0.9751\n",
            "Epoch 37/40\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0524 - acc: 0.9813 - auc: 0.9978 - val_loss: 1.6062 - val_acc: 0.6483 - val_auc: 0.8135\n",
            "Epoch 38/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0502 - acc: 0.9817 - auc: 0.9981 - val_loss: 1.0140 - val_acc: 0.7009 - val_auc: 0.9129\n",
            "Epoch 39/40\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0484 - acc: 0.9817 - auc: 0.9983 - val_loss: 0.2645 - val_acc: 0.9078 - val_auc: 0.9709\n",
            "Epoch 40/40\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0477 - acc: 0.9833 - auc: 0.9981 - val_loss: 0.4383 - val_acc: 0.8509 - val_auc: 0.9603\n",
            "INFO:tensorflow:Assets written to: my_model_not_pretrained/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCqsVqFv5OCM"
      },
      "source": [
        "### train (inception v3 pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3swsOGlWM66M"
      },
      "source": [
        "# model inception V3\n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(299, 299, 3),\n",
        "                                                 include_top=False, \n",
        "                                                 weights='imagenet'\n",
        "                                                #  weights=None\n",
        "                                               )\n",
        "\n",
        "\n",
        "base_model.trainable = True \n",
        "# \n",
        "\n",
        "dropout_layer = layers.Dropout(0.5)\n",
        "gloabal_average_layer = layers.GlobalAveragePooling2D()\n",
        "prediction_layer = layers.Dense(1, activation='sigmoid')\n",
        "###\n",
        "\n",
        "my_model_pretrained = models.Sequential((base_model, \n",
        "                              dropout_layer,\n",
        "                            gloabal_average_layer, \n",
        "                              prediction_layer))\n",
        "\n",
        "my_model_pretrained.compile(loss='binary_crossentropy',\n",
        "              # metrics=['acc'],\n",
        "              metrics=['acc', tf.keras.metrics.AUC()],\n",
        "              optimizer='sgd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hfQl4YJ5xzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87abad4-9914-42f9-9707-166059c046c7"
      },
      "source": [
        "history = my_model_pretrained.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=121,\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=10)\n",
        "\n",
        "# \n",
        "my_model.save(\"my_model_pretrained\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "121/121 [==============================] - 326s 3s/step - loss: 0.3452 - acc: 0.8580 - auc_1: 0.9301 - val_loss: 0.2809 - val_acc: 0.8853 - val_auc_1: 0.9805\n",
            "Epoch 2/20\n",
            "121/121 [==============================] - 324s 3s/step - loss: 0.0989 - acc: 0.9659 - auc_1: 0.9939 - val_loss: 0.2298 - val_acc: 0.9112 - val_auc_1: 0.9736\n",
            "Epoch 3/20\n",
            "121/121 [==============================] - 320s 3s/step - loss: 0.0735 - acc: 0.9744 - auc_1: 0.9964 - val_loss: 0.2438 - val_acc: 0.8974 - val_auc_1: 0.9656\n",
            "Epoch 4/20\n",
            "121/121 [==============================] - 319s 3s/step - loss: 0.0594 - acc: 0.9788 - auc_1: 0.9977 - val_loss: 0.2755 - val_acc: 0.8888 - val_auc_1: 0.9604\n",
            "Epoch 5/20\n",
            "121/121 [==============================] - 320s 3s/step - loss: 0.0484 - acc: 0.9836 - auc_1: 0.9984 - val_loss: 0.2481 - val_acc: 0.9026 - val_auc_1: 0.9674\n",
            "Epoch 6/20\n",
            "121/121 [==============================] - 322s 3s/step - loss: 0.0492 - acc: 0.9818 - auc_1: 0.9982 - val_loss: 0.2635 - val_acc: 0.9017 - val_auc_1: 0.9663\n",
            "Epoch 7/20\n",
            "121/121 [==============================] - 324s 3s/step - loss: 0.0403 - acc: 0.9849 - auc_1: 0.9990 - val_loss: 0.3002 - val_acc: 0.8888 - val_auc_1: 0.9656\n",
            "Epoch 8/20\n",
            "121/121 [==============================] - 319s 3s/step - loss: 0.0381 - acc: 0.9855 - auc_1: 0.9992 - val_loss: 0.2997 - val_acc: 0.8897 - val_auc_1: 0.9614\n",
            "Epoch 9/20\n",
            "121/121 [==============================] - 317s 3s/step - loss: 0.0353 - acc: 0.9873 - auc_1: 0.9992 - val_loss: 0.2548 - val_acc: 0.9086 - val_auc_1: 0.9665\n",
            "Epoch 10/20\n",
            "121/121 [==============================] - 318s 3s/step - loss: 0.0305 - acc: 0.9892 - auc_1: 0.9994 - val_loss: 0.2884 - val_acc: 0.9009 - val_auc_1: 0.9651\n",
            "Epoch 11/20\n",
            "121/121 [==============================] - 325s 3s/step - loss: 0.0303 - acc: 0.9893 - auc_1: 0.9993 - val_loss: 0.2443 - val_acc: 0.9164 - val_auc_1: 0.9715\n",
            "Epoch 12/20\n",
            "121/121 [==============================] - 318s 3s/step - loss: 0.0276 - acc: 0.9909 - auc_1: 0.9995 - val_loss: 0.2552 - val_acc: 0.9181 - val_auc_1: 0.9670\n",
            "Epoch 13/20\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0271 - acc: 0.9896 - auc_1: 0.9996 - val_loss: 0.2642 - val_acc: 0.9155 - val_auc_1: 0.9674\n",
            "Epoch 14/20\n",
            "121/121 [==============================] - 317s 3s/step - loss: 0.0264 - acc: 0.9907 - auc_1: 0.9996 - val_loss: 0.2879 - val_acc: 0.9069 - val_auc_1: 0.9701\n",
            "Epoch 15/20\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0235 - acc: 0.9917 - auc_1: 0.9994 - val_loss: 0.2501 - val_acc: 0.9224 - val_auc_1: 0.9713\n",
            "Epoch 16/20\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0227 - acc: 0.9919 - auc_1: 0.9997 - val_loss: 0.2447 - val_acc: 0.9181 - val_auc_1: 0.9726\n",
            "Epoch 17/20\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0231 - acc: 0.9922 - auc_1: 0.9993 - val_loss: 0.3034 - val_acc: 0.9069 - val_auc_1: 0.9699\n",
            "Epoch 18/20\n",
            "121/121 [==============================] - 316s 3s/step - loss: 0.0229 - acc: 0.9916 - auc_1: 0.9995 - val_loss: 0.2573 - val_acc: 0.9198 - val_auc_1: 0.9715\n",
            "Epoch 19/20\n",
            "121/121 [==============================] - 314s 3s/step - loss: 0.0204 - acc: 0.9925 - auc_1: 0.9998 - val_loss: 0.3044 - val_acc: 0.9034 - val_auc_1: 0.9736\n",
            "Epoch 20/20\n",
            "121/121 [==============================] - 315s 3s/step - loss: 0.0178 - acc: 0.9928 - auc_1: 0.9998 - val_loss: 0.3043 - val_acc: 0.9086 - val_auc_1: 0.9697\n",
            "INFO:tensorflow:Assets written to: my_model_pretrained/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyX_6EJi_fNT"
      },
      "source": [
        "### produce a heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZG-E2lb_dsu"
      },
      "source": [
        "def produce_heatmap(slide_path, tumor_mask_path, model, level=2):\n",
        "    slide = open_slide(slide_path)\n",
        "    tumor_mask = open_slide(tumor_mask_path)\n",
        "    # slide a window across the slide\n",
        "    window_width = 299\n",
        "    window_height = 299\n",
        "    stride = 128\n",
        "    # \n",
        "    slide_width, slide_height = slide.level_dimensions[level]\n",
        "    # num of grids\n",
        "    nx = (slide_width - window_width) // stride + 1\n",
        "    ny = (slide_height - window_height) // stride + 1\n",
        "    \n",
        "    # \n",
        "    image_input = np.zeros((nx*ny, 299, 299, 3), dtype=np.float)\n",
        "    labels_list = []\n",
        "    k = 0\n",
        "    for i in range(ny):\n",
        "        for j in range(nx):\n",
        "            x, y = j*stride, i*stride\n",
        "            region = read_slide(slide, x=x * 2**level, y = y * 2**level, \n",
        "                               level=level, width=window_width, height=window_height)\n",
        "            image_input[k, :, :, :] = region\n",
        "            mask_region = read_slide(tumor_mask, x=x * 2**level, y = y * 2**level, \n",
        "                               level=level, width=window_width, height=window_height)[:,:,0]\n",
        "            label = mask_region[86:214, 86:214].astype(np.float).max()\n",
        "            labels_list.append(label)\n",
        "            k += 1\n",
        "    # predict\n",
        "    image_input = image_input.astype(np.float32)/255 \n",
        "    pred_labels = model.predict_classes(image_input).ravel()\n",
        "    labels_list = np.array(labels_list)\n",
        "    print('accuracy: ', (pred_labels==labels_list).astype(int).mean())\n",
        "#     print(type(pred_labels))\n",
        "    # produce heatmap\n",
        "    # '''\n",
        "    print(pred_labels.shape)\n",
        "    print(pred_labels.min())\n",
        "    print(pred_labels.max())\n",
        "    heatmap = np.zeros((slide_height, slide_width), dtype=np.uint8)\n",
        "    \n",
        "    k = 0\n",
        "    for i in range(ny):\n",
        "        for j in range(nx):\n",
        "            x, y = j*stride, i*stride\n",
        "            heatmap[x+85: x+213, y+85: y+213] = pred_labels[k]\n",
        "            k += 1\n",
        "    tumor_mask_image = read_slide(tumor_mask, x=0, y=0, level=level, \n",
        "                                  width=slide_width, height=slide_height)[:, :, 0]\n",
        "    # '''\n",
        "    return heatmap, tumor_mask_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3GMMlDy5IclQ",
        "outputId": "ffe3dc2f-5177-472d-c089-a9648c32e32e"
      },
      "source": [
        "# heatmap for training set\n",
        "'''\n",
        "slide_path = 'slides/tumor_075.tif'\n",
        "tumor_mask_path = 'slides/tumor_075_mask.tif'\n",
        "heatmap, tumor_mask_image = produce_heatmap(slide_path, tumor_mask_path, model=my_model, level=4)\n",
        "\n",
        "#plot \n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.subplot(121)\n",
        "plt.imshow(heatmap)\n",
        "plt.subplot(122)\n",
        "plt.imshow(tumor_mask_image)\n",
        "\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nslide_path = 'slides/tumor_075.tif'\\ntumor_mask_path = 'slides/tumor_075_mask.tif'\\nheatmap, tumor_mask_image = produce_heatmap(slide_path, tumor_mask_path, model=my_model, level=4)\\n\\n#plot \\nplt.figure(figsize=(20, 10))\\nplt.subplot(121)\\nplt.imshow(heatmap)\\nplt.subplot(122)\\nplt.imshow(tumor_mask_image)\\n\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vPe2w5ahXfa"
      },
      "source": [
        "## Train with multiple zoom levels (level 2 and level 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW0l5O21heum"
      },
      "source": [
        "### Build a Multi-inputs Model (dropout used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIY3BDIahd39"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# use tensorflow functional API to build the model \n",
        "\n",
        "def MyModel():\n",
        "  inputs_L2 = layers.Input(shape=(299, 299, 3,))\n",
        "  inputs_L4 = layers.Input(shape=(299, 299, 3,))\n",
        "  v3_L2 = InceptionV3(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\n",
        "  v3_L4 = InceptionV3(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\n",
        "  # rename\n",
        "  v3_L2._name += \"L2\"\n",
        "  v3_L4._name += \"L4\"\n",
        "  #\n",
        "\n",
        "  L2 = v3_L2(inputs_L2)\n",
        "  L4 = v3_L4(inputs_L4)\n",
        "  # Dropout\n",
        "  L2 = layers.Dropout(0.3)(L2) ############\n",
        "  L4 = layers.Dropout(0.3)(L4) \n",
        "  # global average\n",
        "  # global_average_layer = layers.GlobalAveragePooling2D()\n",
        "  L2 = layers.GlobalAveragePooling2D()(L2)\n",
        "  L4 = layers.GlobalAveragePooling2D()(L4)\n",
        "  # print(L2.shape)\n",
        "  # prediction\n",
        "  prediction_layer = layers.Dense(1, activation='sigmoid')\n",
        "  outputs = prediction_layer(tf.concat((L2, L4), axis=1))\n",
        "  # model\n",
        "  model = models.Model(inputs=[inputs_L2, inputs_L4], outputs=outputs)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                metrics=['acc', tf.keras.metrics.AUC()],\n",
        "                optimizer='sgd')\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI6AQxpG9s8D"
      },
      "source": [
        "### load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYvaJyZQ8kiA"
      },
      "source": [
        "import random\n",
        "# dir\n",
        "L2_train_dir = 'dataset_L2/train'\n",
        "L2_validation_dir = 'dataset_L2/validation'\n",
        "L4_train_dir = 'dataset_L4/train'\n",
        "L4_validation_dir = 'dataset_L4/validation'\n",
        "# \n",
        "def get_paths_and_labels(L2_dir):\n",
        "    neg_dir_L2 = os.path.join(L2_dir, 'negative')  # negative label is 0\n",
        "    pos_dir_L2 = os.path.join(L2_dir, 'positive')  # positive label is 1\n",
        "    # \n",
        "    neg_images_path_L2 = os.listdir(neg_dir_L2)\n",
        "    # print(neg_images_path_L2[:10])\n",
        "    neg_images_path_L2 = [os.path.join(neg_dir_L2, i) for i in neg_images_path_L2]\n",
        "    \n",
        "    pos_images_path_L2 = os.listdir(pos_dir_L2)\n",
        "    # print(pos_images_path_L2[:10])\n",
        "    pos_images_path_L2 = [os.path.join(pos_dir_L2, i) for i in pos_images_path_L2]\n",
        "    \n",
        "    images_path_L2 = neg_images_path_L2 + pos_images_path_L2\n",
        "    # print(images_path_L2[:10])\n",
        "    # \n",
        "    neg_labels = [0 for _ in range(len(neg_images_path_L2))]\n",
        "    pos_labels = [1 for _ in range(len(pos_images_path_L2))]\n",
        "    labels = neg_labels + pos_labels\n",
        "    \n",
        "    # # L4 \n",
        "    images_path_L4 = [p.replace('L2', 'L4') for p in images_path_L2]\n",
        "    # print(images_path_L4[:10])\n",
        "    \n",
        "    # shuffle \n",
        "    path_and_labels = list(zip(images_path_L2, images_path_L4, labels))\n",
        "    random.shuffle(path_and_labels)\n",
        "    images_path_L2, images_path_L4, labels = zip(*path_and_labels)\n",
        "    \n",
        "    \n",
        "    return images_path_L2, images_path_L4, labels  # x1, x2, y\n",
        "\n",
        "# def \n",
        "train_L2_paths, train_L4_paths, train_labels = get_paths_and_labels(L2_train_dir)\n",
        "validation_L2_paths, validation_L4_paths, validation_labels = get_paths_and_labels(L2_validation_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZSbOenpfTA_"
      },
      "source": [
        "#### Build an input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ZtFzB43cuq"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  img = tf.image.resize(img, [299, 299])\n",
        "  img /= 255.0  # normalize pixels to 0,1\n",
        "  return img\n",
        "\n",
        "def input_pipeline_images(paths):\n",
        "  paths = np.array(paths)\n",
        "  path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "  # image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "  image_ds = path_ds.map(load_and_preprocess_image)\n",
        "  return image_ds\n",
        "\n",
        "\n",
        "train_ds_L2 = input_pipeline_images(train_L2_paths)\n",
        "train_ds_L4 = input_pipeline_images(train_L4_paths)\n",
        "train_labels_ds = tf.data.Dataset.from_tensor_slices(np.array(train_labels))\n",
        "# \n",
        "validation_ds_L2 = input_pipeline_images(validation_L2_paths)\n",
        "validation_ds_L4 = input_pipeline_images(validation_L4_paths)\n",
        "validation_labels_ds = tf.data.Dataset.from_tensor_slices(np.array(validation_labels))\n",
        "\n",
        "\n",
        "\n",
        "########\n",
        "train_ds = tf.data.Dataset.zip(((train_ds_L2, train_ds_L4), train_labels_ds))\n",
        "validation_ds = tf.data.Dataset.zip(((validation_ds_L2, validation_ds_L4), validation_labels_ds))\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "validation_ds = validation_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmrCXlH3HqTX"
      },
      "source": [
        "### data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJtPgkDxH41W"
      },
      "source": [
        "def perform_data_augmentation(train_ds):\n",
        "  data_augmentation = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "    layers.experimental.preprocessing.RandomRotation(0.25), # 90 degrees\n",
        "  ])\n",
        "\n",
        "  train_ds_augmented = train_ds.map(lambda x, y: ((data_augmentation(x[0]),\n",
        "                                                   data_augmentation(x[1])),\n",
        "                                                   y),\n",
        "                              num_parallel_calls=AUTOTUNE  \n",
        "  )\n",
        "  return train_ds_augmented\n",
        "\n",
        "  ##\n",
        "  # train_ds = perform_data_augmentation(train_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3pgS_SRH6-j"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnq-NQDEHlSC",
        "outputId": "dede4632-94da-41c8-b2af-d920d80cf3b1"
      },
      "source": [
        "model_multiple = MyModel()\n",
        "########\n",
        "# i found that this model converges too fast, so i used an early stop to avoid overfitting \n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
        "\n",
        "#######\n",
        "epochs = 20\n",
        "history_multi = model_multiple.fit(x=train_ds, validation_data=validation_ds, validation_steps=1160//BATCH_SIZE + 1, epochs=epochs, callbacks=[callback])\n",
        "\n",
        "model_multiple.save(\"model_multiple_zoom\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "242/242 [==============================] - 7252s 30s/step - loss: 0.2821 - acc: 0.8818 - auc_2: 0.9509 - val_loss: 0.1753 - val_acc: 0.9414 - val_auc_2: 0.9845\n",
            "Epoch 2/20\n",
            "242/242 [==============================] - 169s 696ms/step - loss: 0.0448 - acc: 0.9863 - auc_2: 0.9990 - val_loss: 0.1901 - val_acc: 0.9216 - val_auc_2: 0.9835\n",
            "INFO:tensorflow:Assets written to: model_multiple_zoom/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsOZbvhXJgeq"
      },
      "source": [
        "## Heatmap & Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDKkIS5OJxXs"
      },
      "source": [
        "### predict and produce heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9OEPZ_nJs61"
      },
      "source": [
        "def predict_labels(slide_path, tumor_mask_path, model, level=2, file_name='prediction', pred_mode='single'):\n",
        "    slide = open_slide(slide_path)\n",
        "    tumor_mask = open_slide(tumor_mask_path)\n",
        "    # slide a window across the slide\n",
        "    window_width = 299\n",
        "    window_height = 299\n",
        "    stride = 128\n",
        "    # \n",
        "    slide_width, slide_height = slide.level_dimensions[level]\n",
        "    # num of grids\n",
        "    nx = (slide_width - window_width) // stride + 1\n",
        "    ny = (slide_height - window_height) // stride + 1\n",
        "    \n",
        "    print('nx:',nx,'ny:',ny, nx*ny)\n",
        "    print('width', slide_width, 'height:', slide_height)\n",
        "    # \n",
        "    image_input = np.zeros((nx*ny, 299, 299, 3), dtype=np.float32)\n",
        "    labels_list = []\n",
        "    k = 0\n",
        "    slide_image = read_slide(slide, x=0, y=0, level=level,\n",
        "                            width=slide_width, height=slide_height)\n",
        "    tumor_mask_image = read_slide(tumor_mask, x=0, y=0, level=level, \n",
        "                                  width=slide_width, height=slide_height)[:, :, 0]\n",
        "    \n",
        "    \n",
        "    ### \n",
        "    if pred_mode == 'multi':\n",
        "        image_input_L4 = image_input.copy()\n",
        "    \n",
        "    ### \n",
        "    for i in range(ny):\n",
        "        for j in range(nx):\n",
        "            x, y = j*stride, i*stride\n",
        "            region = slide_image[y:y+window_height, x:x+window_width, :]\n",
        "            image_input[k, :, :, :] = region\n",
        "            ### \n",
        "            if pred_mode == 'multi':\n",
        "                level_L4 = level + 2\n",
        "                region_L4 = read_slide(slide, x=(x - window_width//2*3)* 2**level,\n",
        "                                    y=(y-window_height//2*3)*2**level, level=level_L4,\n",
        "                                    width=window_width, height=window_height)\n",
        "                image_input_L4[k, :, :, :] = region_L4\n",
        "            \n",
        "            mask_region = tumor_mask_image[y:y+window_height, x:x+window_width]\n",
        "            label = mask_region[86:214, 86:214].astype(np.float).max()\n",
        "            labels_list.append(label)\n",
        "            k += 1\n",
        "            \n",
        "            \n",
        "    # predict\n",
        "    image_input = image_input/255 \n",
        "    if pred_mode == 'multi':\n",
        "        image_input_L4 /= 255\n",
        "        pred_labels = model.predict([image_input, image_input_L4]).ravel()\n",
        "    elif pred_mode == 'single':\n",
        "        pred_labels = model.predict(image_input).ravel() # probabilities\n",
        "    # ground truth\n",
        "    labels_list = np.array(labels_list).astype(np.float32)\n",
        "    \n",
        "    # save\n",
        "    all_labels = np.vstack((pred_labels, labels_list))\n",
        "    np.savetxt(file_name, all_labels)\n",
        "    return all_labels\n",
        "    \n",
        "    \n",
        "def predict_testset(model, file_name, pred_mode):\n",
        "    slide_paths = ['slides/tumor_{}.tif'.format(i) for i in ['096', '101', '110']]\n",
        "    tumor_mask_paths = ['slides/tumor_{}_mask.tif'.format(i) for i in ['096', '101', '110']]\n",
        "    \n",
        "    # \n",
        "    for i in range(3):\n",
        "        slide_path = slide_paths[i]\n",
        "        tumor_mask_path = tumor_mask_paths[i]\n",
        "        test_labels = predict_labels(slide_path, tumor_mask_path, model, file_name=file_name, \n",
        "                                        pred_mode=pred_mode)\n",
        "        if i == 0:\n",
        "            final_labels = test_labels\n",
        "        else:\n",
        "            final_labels = np.hstack((final_labels, test_labels))\n",
        "    return final_labels\n",
        "\n",
        "# model = model_multiple\n",
        "# final_labels = predict_testset(model_multiple, file_name='model_multi_prediction', pred_mode='multi')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3gS1Aasqc0X"
      },
      "source": [
        "# produce heatmap\n",
        "\n",
        "def produce_heatmap(all_labels, height, width, nx, ny, slide_path, image_name='00.png'):\n",
        "    heatmap = np.zeros((height, width), dtype=np.uint8)\n",
        "    stride = 128\n",
        "    \n",
        "    pred_labels = all_labels[0, :]\n",
        "    pred_labels = (pred_labels>0.5).astype(int)\n",
        "    \n",
        "    k = 0\n",
        "    for i in range(ny):\n",
        "        for j in range(nx):\n",
        "            x, y = j*stride, i*stride\n",
        "            heatmap[y+86: y+214, x+86: x+214] = pred_labels[k]\n",
        "            k += 1\n",
        "\n",
        "    # \n",
        "    slide = open_slide(slide_path)\n",
        "    slide_image = read_slide(slide, x=0, y=0, level=5, \n",
        "                                  width=slide.level_dimensions[5][0],\n",
        "                                  height=slide.level_dimensions[5][1])\n",
        "    \n",
        "    slide_image = Image.fromarray(slide_image)\n",
        "    heatmap = Image.fromarray(heatmap)\n",
        "    heatmap = heatmap.resize(slide_image.size)\n",
        "    heatmap.save(image_name)\n",
        "    ## plt \n",
        "    fig = plt.figure(figsize=(14, 14))\n",
        "    plt.imshow(slide_image)\n",
        "    plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.savefig(image_name)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rhazWU8J26c"
      },
      "source": [
        "### metrics (AUC, acc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlnQtuEGJ2NC"
      },
      "source": [
        "def compute_AUC(final_labels):\n",
        "    pred_labels = final_labels[0, :]\n",
        "    real_labels = final_labels[1, :].astype(np.int)\n",
        "    \n",
        "    m = tf.keras.metrics.AUC()\n",
        "    m.update_state(real_labels, pred_labels)\n",
        "    auc = m.result().numpy()\n",
        "    return auc\n",
        "\n",
        "def compute_acc(final_labels):\n",
        "    pred_labels = final_labels[0, :]\n",
        "    real_labels = final_labels[1, :].astype(np.int)\n",
        "    \n",
        "    pred_labels = (pred_labels>0.5).astype(int)\n",
        "    \n",
        "    acc = ((pred_labels == real_labels).astype(int)).sum() / len(pred_labels)\n",
        "    return acc\n",
        "\n",
        "# AUC = compute_AUC(final_labels)\n",
        "# acc = compute_acc(final_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALVW6xViTp52"
      },
      "source": [
        "### Results (heatmaps & metrics shown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oet8BJc89Imi"
      },
      "source": [
        "#### heatmaps\n",
        "* slide 096 prediction\n",
        "![](https://github.com/LongfeiShen/DL/blob/main/096.png?raw=true)\n",
        "* slide 101 prediction\n",
        "![](https://github.com/LongfeiShen/DL/blob/main/101.png?raw=true)\n",
        "* slide 110 prediction\n",
        "![](https://github.com/LongfeiShen/DL/blob/main/110.png?raw=true)\n",
        "\n",
        "#### metrics\n",
        "![](https://github.com/LongfeiShen/DL/blob/main/metrics.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69_8RLotR2S"
      },
      "source": [
        "## script produce heatmap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Ki7xiHtU7P"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import tensorflow as tf\n",
        "\n",
        "def produce_heatmap_with_slide(slide_path, model, level=2, pred_mode='single'):\n",
        "    slide = open_slide(slide_path)\n",
        "    # slide a window across the slide\n",
        "    window_width = 299\n",
        "    window_height = 299\n",
        "    stride = 128\n",
        "    # \n",
        "    slide_width, slide_height = slide.level_dimensions[level]\n",
        "    # num of grids\n",
        "    nx = (slide_width - window_width) // stride + 1\n",
        "    ny = (slide_height - window_height) // stride + 1\n",
        "    # \n",
        "    image_input = np.zeros((nx*ny, 299, 299, 3), dtype=np.float32)\n",
        "    k = 0\n",
        "    slide_image = read_slide(slide, x=0, y=0, level=level,\n",
        "                            width=slide_width, height=slide_height)\n",
        "\n",
        "    ### \n",
        "    if pred_mode == 'multi':\n",
        "        image_input_L4 = image_input.copy()\n",
        "    \n",
        "    ### \n",
        "    for i in range(ny):\n",
        "        for j in range(nx):\n",
        "            x, y = j*stride, i*stride\n",
        "            region = slide_image[y:y+window_height, x:x+window_width, :]\n",
        "            image_input[k, :, :, :] = region\n",
        "            ### \n",
        "            if pred_mode == 'multi':\n",
        "                level_L4 = level + 2\n",
        "                region_L4 = read_slide(slide, x=(x - window_width//2*3)* 2**level,\n",
        "                                    y=(y-window_height//2*3)*2**level, level=level_L4,\n",
        "                                    width=window_width, height=window_height)\n",
        "                image_input_L4[k, :, :, :] = region_L4\n",
        "            k += 1\n",
        "            \n",
        "            \n",
        "    # predict\n",
        "    image_input = image_input/255 \n",
        "    print(\"predicting...\")\n",
        "    if pred_mode == 'multi':\n",
        "        image_input_L4 /= 255\n",
        "        pred_labels = model.predict([image_input, image_input_L4]).ravel()\n",
        "    elif pred_mode == 'single':\n",
        "        pred_labels = model.predict(image_input).ravel()\n",
        "    # produce heatmap\n",
        "    pred_labels = (pred_labels > 0.5).astype(np.int)\n",
        "    pred_labels = pred_labels.reshape((ny, nx)).astype(np.uint8)\n",
        "    plt.imshow(pred_labels)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "YA7lspzJNX94",
        "outputId": "445ec554-051b-4dc8-a743-1f6e306c350f"
      },
      "source": [
        "slide_path = \"slides/tumor_110.tif\"\n",
        "model = models.load_model(\"model_multiple_zoom\")\n",
        "\n",
        "# predition mode, 'single' if single input model\n",
        "# 'multi' if multi-inputs model\n",
        "pred_mode=\"multi\"\n",
        "# best results when level = 2,\n",
        "# but takes much more time and sometimes crashes in colab\n",
        "# so level = 3 is recommended. \n",
        "# level 4 or even lower levels can not perform well \n",
        "level = 3  \n",
        "\n",
        "produce_heatmap_with_slide(slide_path, model, level=level, pred_mode=pred_mode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicting...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAAD7CAYAAAD0KCaWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARHElEQVR4nO3dX6xlZXnH8e+vwz9BEUbpZGRIoZFASFPATvgTjUEQpdZILwyBmmbakMyNbbE1EWiTpia90KRRuWhMJqKSxiIUtRBiVBghTZsGGQQVGBBEkKEDgwaqbRMEfXqx17TH4Zwza87ea52zz/v9JDt7rbX3Pu+z/8wz7/Outd6VqkKSWvVrqx2AJK0mk6CkppkEJTXNJCipaSZBSU0zCUpq2lRJMMklSR5N8niSa2YVlCSNJSs9TjDJBuD7wMXAHuBe4Iqqenh24UnSsA6b4rXnAI9X1RMASb4IXAosmQSPyJF1FMdM0aQkrczPeOHHVXXCgdunSYInAk8vWN8DnLvcC47iGM7NRVM0KUkrc2fd8tRi26dJgr0k2Q5sBziKo4duTpIOyTQ7Rp4BTlqwvqXb9iuqakdVba2qrYdz5BTNSdLsTZME7wVOTXJKkiOAy4HbZhOWJI1jxeVwVb2S5E+ArwMbgM9W1UMzi0ySRjDVmGBVfRX46oxikaTRecaIpKaZBCU1zSQoqWkmQUlNMwlKappJUFLTTIKSmmYSlNQ0k6CkppkEJTXNJCipaSZBSU0zCUpqmklQUtNMgpKaZhKU1DSToKSmmQQlNc0kKKlpg193WOvT1//jgf9bfvebzur92MH+1nL6/C3pUNkTlNQ0k6CkplkOa0UWlqYHlrNDlcBLvcYyWdM4aE8wyWeT7Evy4IJtG5PckeSx7v74YcOUpGH0KYc/D1xywLZrgJ1VdSqws1uXpLlz0HK4qv4lyckHbL4UuKBbvgG4G7h6hnFpDZq2HF1qr/Fy5XTfsrlPm0uxnG7bSneMbKqqvd3ys8CmGcUjSaOaeu9wVRVQSz2eZHuSXUl2vcxL0zYnSTO10r3DzyXZXFV7k2wG9i31xKraAewAODYbl0yWWvuWKlNXUrIu95o+ZXPfknmWpbXWp5X2BG8DtnXL24BbZxOOJI2rzyEyNwL/DpyWZE+SK4GPARcneQx4Z7cuSXOnz97hK5Z46KIZx6IRHer5vSs1bTk6y7277gXWYjxtTlLTTIKSmmYSlNQ0J1BoyGofIjIvh6scamyONc43e4KSmmYSlNQ0y2GtaD7AWU6AMO30/NNaSfyWwOuHPUFJTTMJSmpaJpPAjOPYbKxz44kma8Esp7ef1nLl8EpiGapUnXYCB62uO+uW+6pq64Hb7QlKappJUFLT3Dus3mY5h99KyuzV3lNrCbw+2ROU1DSToKSmWQ7PkVmWgyt5zUrK1qH2Lq9GyTnt+9faZE9QUtNMgpKaZhKU1DTHBNeB1Z6bb8jxxWnbmaWl2l/tz1/TsScoqWkmQUlNsxyeI33LsXmZxn6/1Z5PcCXm4XNVP30uvn5SkruSPJzkoSRXdds3JrkjyWPd/fHDhytJs9WnHH4F+HBVnQGcB3wwyRnANcDOqjoV2NmtS9JcOWg5XFV7gb3d8s+S7AZOBC4FLuiedgNwN3D1IFGuQ7PcO7pcybgapfFKptSfN/M25KClHdKOkSQnA2cD9wCbugQJ8CywaaaRSdIIeifBJK8FvgR8qKp+uvCxmkxPvegU1Um2J9mVZNfLvDRVsJI0a732Dic5nEkC/EJVfbnb/FySzVW1N8lmYN9ir62qHcAOmEyvP4OY58pql0pDTVqw8O/O+j0u1c5aLaGX27s9D/G3rs/e4QDXA7ur6hMLHroN2NYtbwNunX14kjSsPj3BtwJ/CHwvyf7/1v4S+Bhwc5IrgaeAy4YJUZKG02fv8L8CWeJhLx0naa55xsga12ccabkzRmZplmN/fcfK1tI42hiTxWp8njssqWkmQUlNsxw+BH3LnmlLuGlfP3Z5ttwhIss9b95YAq9P9gQlNc0kKKlpmZzxNo5js7HOzfo4qmascmjaqetXUsLNe9k6hnm8PEDr7qxb7quqrQdutycoqWkmQUlNc+/wQUxb9syybBrrZPxp21mvkwasZJhhPb3/9cqeoKSmmQQlNc1y+CD6zG13YDk01J7jvqXVapdgq93+2JZ7v+t1aGA9sScoqWkmQUlNMwlKappjgiPqc+jLSkz7+r7XyOj7+vWqlffZGnuCkppmEpTUNCdQOIjVPtxlNfQ5FGgtxy8txgkUJGkRJkFJTbMcPgSz3gur9ck96mvTisvhJEcl+VaS7yR5KMlHu+2nJLknyeNJbkpyxBCBS9KQ+pTDLwEXVtWZwFnAJUnOAz4OfLKq3gy8AFw5XJiSNIyDHixdk3r5v7rVw7tbARcCf9BtvwH4G+DTsw9x7VvuYGPLnvXPyxbMt147RpJsSPIAsA+4A/gB8GJVvdI9ZQ9w4jAhStJweiXBqvpFVZ0FbAHOAU7v20CS7Ul2Jdn1Mi+tMExJGsYhnTtcVS8muQs4HzguyWFdb3AL8MwSr9kB7IDJ3uEp411VTqmuPvz+50ufvcMnJDmuW34NcDGwG7gLeH/3tG3ArUMFKUlD6dMT3AzckGQDk6R5c1XdnuRh4ItJ/ha4H7h+wDglaRB99g5/Fzh7ke1PMBkflKS55XyCK+S4j7Q+eO6wpKaZBCU1zXJYmjHPGJov9gQlNc0kKKlplsPSlJabQENrnz1BSU0zCUpqmuWwNCXL3/lmT1BS00yCkppmOSwto8+F6DXf7AlKappJUFLTTIKSmuaYoLQCToywftgTlNQ0k6CkplkOS8uw7F3/7AlKappJUFLTTIKSmtY7CSbZkOT+JLd366ckuSfJ40luSnLEcGFK0jAOpSd4FbB7wfrHgU9W1ZuBF4ArZxmYJI2hVxJMsgX4PeAz3XqAC4FbuqfcAPz+EAFK0pD69gQ/BXwE+GW3/gbgxap6pVvfA5w449gkaXAHTYJJ3gvsq6r7VtJAku1JdiXZ9TIvreRPSNJg+hws/VbgfUneAxwFHAtcBxyX5LCuN7gFeGaxF1fVDmAHwLHZWDOJWpJm5KA9waq6tqq2VNXJwOXAN6vqA8BdwPu7p20Dbh0sSkkayDTHCV4N/EWSx5mMEV4/m5AkaTyHdO5wVd0N3N0tPwGcM/uQJGk8TqAwoqWuV7HS50manqfNSWqaSVBS0yyHR9T3ko2WwNJ47AlKappJUFLTLIfXgFmXv+5dlvqzJyipaSZBSU2zHB7YUqXptGXqciWvJbDUnz1BSU0zCUpqmuXwAJY7EHo1/5akV7MnKKlpJkFJTTMJSmqaY4IDmOXhL7P8u5JezZ6gpKaZBCU1zXJ4DZq3svfA8n3e4lfb7AlKappJUFLTLIdxqvsD9TlLpcXPRetTrySY5EngZ8AvgFeqamuSjcBNwMnAk8BlVfXCMGFK0jAOpRx+R1WdVVVbu/VrgJ1VdSqws1uXpLkyTTl8KXBBt3wDcDdw9ZTxaMaGKm2X+7tO76950rcnWMA3ktyXZHu3bVNV7e2WnwU2zTw6SRpY357g26rqmSS/DtyR5JGFD1ZVJanFXtglze0AR3H0VMFK0qz1SoJV9Ux3vy/JV4BzgOeSbK6qvUk2A/uWeO0OYAfAsdm4aKJcbZZsUrsOWg4nOSbJ6/YvA+8CHgRuA7Z1T9sG3DpUkJI0lD49wU3AV5Lsf/4/VtXXktwL3JzkSuAp4LLhwpSkYRw0CVbVE8CZi2z/CXDREEFJ0lg8Y0SrzgkYtJo8d1hS00yCkppmOTyn+p6VMVRpOctLCFj+ajXZE5TUNJOgpKZZDs+RpSYtWI29q9OWs5bAWivsCUpqmklQUtMsh9e4PmVnnzkDD4WlqlpiT1BS00yCkppmEpTUNMcE55TX+JBmw56gpKaZBCU1zXJ4jVuqnJ32MpmWydKEPUFJTTMJSmqa5fCcmvVZIlKr7AlKappJUFLTLIfnyFIl8IF7ei2Vpf569QSTHJfkliSPJNmd5PwkG5PckeSx7v74oYOVpFnrWw5fB3ytqk5nciH23cA1wM6qOhXY2a1L0lw5aDmc5PXA24E/AqiqnwM/T3IpcEH3tBuAu4GrhwhSE05jL81en57gKcDzwOeS3J/kM0mOATZV1d7uOc8Cm4YKUpKG0icJHga8Bfh0VZ0N/DcHlL5VVUAt9uIk25PsSrLrZV6aNl5Jmqk+SXAPsKeq7unWb2GSFJ9Lshmgu9+32IurakdVba2qrYdz5CxilqSZOeiYYFU9m+TpJKdV1aPARcDD3W0b8LHu/tZBI1VvjgNK/fU9TvBPgS8kOQJ4AvhjJr3Im5NcCTwFXDZMiJI0nF5JsKoeALYu8tBFsw1HksblaXOSmmYSlNQ0k6CkppkEJTXNJCipaSZBSU0zCUpqmklQUtNMgpKaZhKU1DSToKSmmQQlNc0kKKlpJkFJTTMJSmqaSVBS00yCkppmEpTUNJOgpKZlcsngkRpLnmdy3eIfj9boq73R9m3f9pts/zeq6oQDN46aBAGS7KqqxS7aZPu2b/u2PzrLYUlNMwlKatpqJMEdq9Cm7du+7dv+okYfE5SktcRyWFLTRk2CSS5J8miSx5NcM0J7n02yL8mDC7ZtTHJHkse6++MHbP+kJHcleTjJQ0muGjOGJEcl+VaS73Ttf7TbfkqSe7rv4aYkRwzR/oI4NiS5P8ntY7ef5Mkk30vyQJJd3bYxfwPHJbklySNJdic5f8Tv/7Tufe+//TTJh0Z+/3/e/fYeTHJj95sc9fd3MKMlwSQbgL8Hfhc4A7giyRkDN/t54JIDtl0D7KyqU4Gd3fpQXgE+XFVnAOcBH+ze81gxvARcWFVnAmcBlyQ5D/g48MmqejPwAnDlQO3vdxWwe8H62O2/o6rOWnBoxpi/geuAr1XV6cCZTD6HUdqvqke7930W8DvA/wBfGav9JCcCfwZsrarfAjYAlzP+97+8qhrlBpwPfH3B+rXAtSO0ezLw4IL1R4HN3fJm4NERP4NbgYtXIwbgaODbwLlMDlY9bLHvZYB2tzD5h3YhcDuQkdt/EnjjAdtG+fyB1wM/pBt7X83fIPAu4N9Gfv8nAk8DG4HDuu//3WN+/31uY5bD+z+Q/fZ028a2qar2dsvPApvGaDTJycDZwD1jxtCVog8A+4A7gB8AL1bVK91Thv4ePgV8BPhlt/6Gkdsv4BtJ7kuyvds21ud/CvA88LluOOAzSY4Zsf2FLgdu7JZHab+qngH+DvgRsBf4T+A+xv3+D6rpHSM1+a9o8N3jSV4LfAn4UFX9dMwYquoXNSmHtgDnAKcP1daBkrwX2FdV943V5iLeVlVvYTIM88Ekb1/44MCf/2HAW4BPV9XZTE4Z/ZXSc4zfYDfm9j7gnw58bMj2u7HGS5n8Z/Am4BhePTy16sZMgs8AJy1Y39JtG9tzSTYDdPf7hmwsyeFMEuAXqurLqxEDQFW9CNzFpPw4Lslh3UNDfg9vBd6X5Engi0xK4utGbH9/b4Sq2sdkPOwcxvv89wB7quqebv0WJklx7O//d4FvV9Vz3fpY7b8T+GFVPV9VLwNfZvKbGO3772PMJHgvcGq3Z+gIJt3z20Zsf7/bgG3d8jYm43SDSBLgemB3VX1i7BiSnJDkuG75NUzGI3czSYbvH7r9qrq2qrZU1clMvu9vVtUHxmo/yTFJXrd/mcm42IOM9PlX1bPA00lO6zZdBDw8VvsLXMH/l8KM2P6PgPOSHN39W9j//kf5/nsbcwASeA/wfSbjUn81Qns3MhmLeJnJ/8pXMhmT2gk8BtwJbByw/bcxKTW+CzzQ3d4zVgzAbwP3d+0/CPx1t/03gW8BjzMpkY4c4bu4ALh9zPa7dr7T3R7a/5sb+TdwFrCr+w7+GTh+5PaPAX4CvH7BtjHb/yjwSPf7+wfgyNX4/S1384wRSU1reseIJJkEJTXNJCipaSZBSU0zCUpqmklQUtNMgpKaZhKU1LT/BZ+XpD45IlWnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJq2xaPS06WE"
      },
      "source": [
        ""
      ]
    }
  ]
}